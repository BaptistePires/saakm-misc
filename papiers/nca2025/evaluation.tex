\section{Evaluation}

\par In this section we evaluate SaaKM ease of use and performance. We presesnt a minimal FIFO scheduler implemented in SaaKM and with the the ext scheduler\cite{schedext} that has been merged in the linux kernel v6.12. Then we compare how both implementations perform on a set of benchmarks. \note{Dans quelle partie mettre ce paragraphe et faut-il le mettre ou l'amener autrement ? -- We chose to compare oursevles to the ext scheduler because it recently has been merged and the others solutions did not fit our needs. Enoki\cite{enoki} works on Linux v5.11, which is 21 release old and 1224 commits behind (for the directory kernel/shed/) the v6.12. Breaking changes also happened to the scheduler, notably the EEVDF merge and ... We want to compare ourselves to kernel scheduler framework and not userspace because it answer different needs, that is why we do not compare SaaKM to ghOSt (which works on a kernel v5.11).}\newline

\subsection{Experimental Setup}
\par \textbf{Benchmark platform} We run our benchmarks on a server running Debien 12 bookworm with a our patched Linux kernel v6.12. The server is equiped with 10 core and 20 SMT threads (Intel(R) Core(TM) i9-10900K CPU @ 3.70GHz), 20MB of L3 cache and 64GO of RAM. \newline

\par \textbf{Benchmarks} We used the Phoronix test suite\cite{phoronix} software to run our benchmarks and the hackbench\cite{hackbench} scheduler benchmark. We run each applications 50 times and clean the kernel caches between each run. We present the mean and the standard deviation of our results. \newline

\par \textbf{Evaluated schedulers} In order to compare ourselves to existing solutions, we implement a FIFO scheduler using SaaKM and sched\_ext. The goal is to have both take the same scheduling decisions to see if our solution add or remove overhead. Each policy has a per-cpu local FIFO runqueue. Threads are always enqueued on the same CPU based on their PID. We do not perform any load balancing to simplify the design and to have more consistent results and have a more deterministic behabior.\note{pas sûr de ça} Only the threads of the benchmarks are switched ot the policies.\newline


% We compare ourselves to the ext scheduler. We implement a minimal FIFO scheduler for comparaison. Functions to select CPUs are based on threads PIDs as e want to mesaure the overhead of SaaKM and not the efficiency of a scheduling algorithm. \\
% We ran a total of X applications, using the phoronix test suite and builtin benchmarks from applications. \newpage

\subsection{Results evaluation}
% \par The goal of our experimenta/tion is to evaluate if SaaKM is a viable solution for writing schedulers and how it performs compared to existing solution. We are motivated by the
\par On average, we see 0.33\% (+/- 1.04\%) of gain with the SaaKM implementation. Table~\ref{tab:bench-res} sums up the results per benchmark. We have a maximum loss with clickhouse for which we avec a decrease of 1.23\% of Queries Per Minute (QPM). The application showing the best results is x265 with a gain of 2.48\% of Frames Per Second (FPS). Out of the 16 benchmarked applications, 10 have a gain or loss between $-0.5\%$ and $0.5\%$. \newline

\par Only two applictions showed more than 1\% loss, the 7zip compression and clickhouse, with respectively -1.2\% and -1.23\%. \note{j'ai des résultats clickhouse avec +0.57\% quand le cache est froid et -0.57\% au second run, là les résultats c'est le 3ème run, je trouve ça plus pertinent de montrer les résultats avec le cache chaud. Mais on peut peut être se servir des deux autres pour montrer que c'est peut être un problème de latence dans saaKM ?}\newline

\subsection{Limitations}
\par Our evaluation shows that SaaKM is a viable solution to write schedulers but it needs deeper tests. In this work we only test a simple per-cpu FIFO scheduler, we need to implement more schedulers such as Shinjuku~\cite{shinjuku}, EEVDF~\cite{eevdf} to see how we perform against them. \newline

\par We did not leverage the multi-policy support of SaaKM, it would be interresting to see how it impacts multiple applications workloads and if the architecture adds overhead due to the policies iteration. \newline

\par x265, Cassandra and npb shows the best improvements, all three gaining more than 1.99\%. 
\begin{itemize}
        \item Definir les métriques
        \item Definir ce à quoi on se compare
        \item Présenter l'env et les benchmarks et les motiver
        \item Présentation et analyse des résultats
\end{itemize}

\note{pour le tableau, est-ce que je laisse comme ça ou je remets les pourcentage sans prendre en compte lower/higher is better et je rajoute une petite colonne pour indiquer la lecture de la ligne ? (lower/higher)}

\begin{table*}[p]
        \centering
        \caption{Benchmarks results. The Gain/Loss column represents the difference between ext and SaaKM in percentage.}
        \label{tab:bench-res}
        \begin{tabular}{|l|l|l|l|}
        % \toprule
        \hline
                Application (metric) &ext & SaaKM & Gain/Loss \\
                \hline
                blender & 141.99 ± 0.52 & 140.25 ± 0.33 & 1.24\% \\
                build-linux-kernel & 215.78 ± 0.63 & 215.52 ± 0.94 & 0.12\% \\
                cassandra & 102686.36 ± 501.57 & 103036.42 ± 596.65 & 0.34\% \\
                clickhouse\_third-run & 126.23 ± 1.4 & 123.43 ± 1.42 & -2.24\% \\
                cloverleaf & 152.81 ($\pm$ 0.34) & 153.09 ($\pm$ 0.45) & 0.18 \\
                compress-7zip\_compress & 58262.14 ± 170.13 & 57350.0 ± 96.53 & -1.58\% \\
                compress-7zip\_decompress & 71097.98 ± 67.71 & 71372.5 ± 33.77 & 0.39\% \\
                dav1d & 188.02 ($\pm$ 0.34) & 188.64 ($\pm$ 0.32) & 0.33 \\
                ffmpeg & 211.36 ($\pm$ 0.52) & 211.49 ($\pm$ 0.40) & 0.06 \\
                hackbench & 33.3 ± 0.03 & 32.75 ± 0.03 & 1.68\% \\
                namd & 0.64 ($\pm$ 0.00) & 0.64 ($\pm$ 0.00) & 0.13 \\
                npb & 21992.45 ± 592.6 & 22089.27 ± 602.4 & 0.44\% \\
                rbenchmark & 0.57 ($\pm$ 0.00) & 0.57 ($\pm$ 0.00) & 0.22 \\
                stockfish & 8392488.30 ($\pm$ 47463.53) & 8427082.62 ($\pm$ 43999.91) & 0.41 \\
                svt-av1 & 95.13 ± 0.05 & 95.08 ± 0.05 & -0.05\% \\
                x265 (FPS) & 57.50 ($\pm$ 1.21) & 58.94 ($\pm$ 1.32) & 2.48\% \\
                \hline
        
        \end{tabular}
\end{table*}

\begin{table*}[p]
        \centering
        \caption{rerun}
        \label{tab:bench-res}
        \begin{tabular}{|l|l|l|l|}
        % \toprule
        \hline
                Application (metric) &ext & SaaKM & Gain/Loss \\
                \hline
blender & 141.99 ± 0.52 & 140.25 ± 0.33 & 1.24\% \\
build-linux-kernel & 215.78 ± 0.63 & 215.52 ± 0.94 & 0.12\% \\
cassandra & 102686.36 ± 501.57 & 103036.42 ± 596.65 & 0.34\% \\
clickhouse\_third-run & 126.23 ± 1.4 & 123.43 ± 1.42 & -2.24\% \\
cloverleaf & 152.88 ± 0.17 & 152.72 ± 0.09 & 0.1\% \\
compress-7zip\_compress & 58262.14 ± 170.13 & 57350.0 ± 96.53 & -1.58\% \\
compress-7zip\_decompress & 71097.98 ± 67.71 & 71372.5 ± 33.77 & 0.39\% \\
dav1d & 187.91 ± 0.15 & 188.72 ± 0.08 & 0.43\% \\
ffmpeg & 211.8 ± 0.15 & 212.19 ± 0.2 & 0.18\% \\
hackbench & 33.3 ± 0.03 & 32.75 ± 0.03 & 1.68\% \\
namd & 0.64 ± 0.0 & 0.64 ± 0.0 & 0.0\% \\
npb & 21992.45 ± 592.6 & 22089.27 ± 602.4 & 0.44\% \\
stockfish & 8389538.38 ± 16078.18 & 8402651.78 ± 21906.34 & 0.16\% \\
svt-av1 & 95.13 ± 0.05 & 95.08 ± 0.05 & -0.05\% \\
x265 & 59.53 ± 0.4 & 61.02 ± 0.39 & 2.46\% \\
rbenchmark & 0.57 ($\pm$ 0.00) & 0.57 ($\pm$ 0.00) & 0.22 \\

                \hline
        
        \end{tabular}
\end{table*}