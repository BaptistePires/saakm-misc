\section{Design}
\label{sec:scheduler-as-a-kernel-module}

% \par We present \textit{Scheduler as a Kernel Module} (SaaKM), a framework that allows kernel developers to write schedulers as Linux kernel modules. Our main goal is to provide a way to write, test and deploy schedulers easily. To do so, we hide the complexity of the core scheduler (synchronization mecanisms, complex API) behind a set of functions corresponding to scheduling events that each scheduling policy must implement. We are also capable to have multiple policies loaded at the same time, allowing each applications to chose the scheduler that best fits its needs.

% \subsection{Design}

% \par In this section we present the disgn behind SaaKM,
\par In this section we present the design of SaaKM, our framework for writing schedulers as Linux kernel modules. First we present the goals of our approach, then we detail the design architeture.

\subsection{Design goals} Writing schedulers in Linux is a tedious task so one of our main goals is to provide a simpler way to do that. The need to have a deep and wide knowledge of the Linux kernel can be a barrier also so we want to hide as much complexity as possible. Finally, testing and debugging a scheduler is also a criteria.

\textbf{Ease of Use} An ideal solution would not require kernel developers to master a new langage nor a new subsystem of the kernel as it would defeat one the purpose of the solution, that is the accessibility part. By the nature of kernel development, they must already be knowledgeable in C and some of the kernel APIs and subsystems. Linux kernel modules answer perfectly to this need.

\textbf{Performance} The solution must incur the lowest overhead possible. As the scheduler is a critiacal part regarding performances, even a small overhead can cause significant performances degradation. We must keep our solution as ligthweiht as possible, giving the users the choice to add features that can cause this overhead if they want to.

\textbf{Testing and Debugging} The current process of debuging and testing a scheduler class require to recompile and redeploy your kernel image each time you make a modification. This can be quite time consuming and slow down the development process. That is why our solution must not be quick and easy to test without the need reboot your machine.
\subsection{Design overview}
\par SaaKM is an API to write schedulers as Linux kernel modules. It exposes a minimal set of exported functions to the modules that can be used to allocate, manage and destroy scheduling policies and data. Figure \ref{fig:sched-class-saakm} shows how SaAKM integrates itself with the current scheduling classes. We position ourselves right before the idle class to not disrupt higher priority classes that other threads runs.\newline

\par All of the SaaKM disgn relies on a structure composed of handlers that each policy must implement. Each handler maps to a scheduling event. Events are devided into two categories. Thread events correspond to all scheduling events related to a thread (e.g threak is created, waking up, ...). On the other hand, core events are related to the CPU cores (e.g scheduling tick, core becoming idle, ...). This distinction makes it clear for the user to know what it should do, hiding the complexity of the scheduling class API where a single function can be called from multiple paths (i.e enqueue\_task). Table \ref{tab:saakm-callbacks} shows an excerpt of those events.\newline

\par Furthermore, this division allows us to hide the complexity of the syucrhonization mecanisms of the core scheduler. As multicore and NUMA architectures are the quite common now, the scheduler must synchroniza data across cores quite often as two core may access the same data. It protects these data through the usage of locks, RCU and low level synchronization mecanisms like memory barriers. Thanks to our design, the user does not need to worry about these and assume that it has the right locks on the data whenever it is needed.\note{réecrire cette partie} \newline

\par To increase the flexibility of our solution, we support to have multiple policies loaded at the same time. This allows users to select the scheduling policy that is tailored for their workload.\note{completer}

\begin{figure}[htbp]
        \centering
        \includesvg[width=0.45\textwidth]{/home/baptiste/these/saakm-misc/papiers/nca2025/figures/linux-saakm-class.svg}
        \caption{SaaKM scheduling class with three policies loaded (refaire pour rendre plus visible)}
        \label{fig:linux-saakm-sched-class}
\end{figure}

\subsection{Implementation}
\par We implement SaaKM on Linux v6.12, the latest longterm release. In this section we will go through the implementation details.

\par We implement a minimal (less than 1500 LoC) scheduling class that implements all of the scheduling functions required by the core scheduler. We place ourselves between the ext and idle scheduling classes as shown in Figure \ref{fig:linux-saakm-sched-class} where there are three policies loaded. We define a structure \texttt{saakm\_module\_routines} that is composed of all the handler a policy must implement. Table \ref{tab:saakm-callbacks} shows an excerpt of these handlers.

\par Each policy must allocate and populate a strcture with its own handlers and they will be called by our scheduling class at the right places. Each handlers has at least one argument that is a pointer to its corresponding policy structure. Then depending on the event type of the handler (thread or core), it will have either a pointer to a structure \texttt{process\_event} or \texttt{core\_event} that contains the data related to the event.

\par \textbf{Runqueues management} We provide a minimal runqueue definition that policies uses and includes in their own runqueues as a structure field. This allows to delegate the insertion/deletion and mandatory runqueue metadata fields to be handled by SaaKM. We provide three runqueue types. A FIFO, LIST and RED-BLACK tree. You must provide an ordering function for each type of runqueue, except for the FIFO that will always insert the task at the end of the queue. We provide these three types as they are the most common ones but thanks to its design it can be extended. Policies are required to initialize their runqueues before registering themselves.\newline 

\par \textbf{Thread states} We define our own thread states to make to not interfer with the core scheduler. We define 7 states summurized in Figure \ref{tab:saakm-states}. We have one special state, SAAKM\_READY\_TICK that allows policy to notify SaaKM that it needs to trigger a reschedule. With this states, we are able to check for wrongful transitions and provide debug feedback to the user at runtime. This ease the debugging and testing process. \note{peut etre faire un diagramme d'etat qui montre la machine à etat et que c'est ça qui nous permet de detecter les erreurs ? mieux que tableau ?}

\par \textbf{Thread Migration} To handle thread migration that occurs because of affinity change or a \texttt{exec} rebalance (a newly created thread is being run on a diffrent CPU than its parent), we introduce a new flag, OUSTED. This flags allows us to know that we are being dequeued becuase of those reason so we can simulate a fast block/unblock sequence to remove the thread from its current runqueue and insert it into the new one. 

\par \textbf{Load balancing} A central and critical part of scheduling is the load balancing. It consist of trying to distribute the load accross CPU cores according to some metrics. To do so, you may need to access data belonging to other cores. Locks are therefore needed to ensure the correctness of the data. We identified two types of load balancing. Periodic load balancing occurs at a fixed time interval and idle balancing. We define a new software interrupt, SCHED\_SOFTIRQ\_IPANEMA with a handler that calls policies \texttt{balancing\_select} function. Policies can them chose at which time interval they want to do load balancing. We also raise our interrupt when the fair scheduling kicks a CPU to do the NOHZ balancing.\note{est ce qu'on doit garder cette feature avec EXT ?} \\
Policies should also be able to do load balacing when a CPU is becoming idle. Whenever that happens, we call the policy handler to let it perform it. 

\par \textbf{Hardware topology} To allow policies to properly make load balancing, we export a per-cpu variable, \texttt{topology\_levels} that represent the underlying hardware toplogy. To do so, we rely on the already existing scheduling domains that are build at initialization of the scheduler subsystem. Policy can then take educated decisions based on the underlying hardware.
% \par Futhermore, this division allows us to hide the complex synchronization mecanisms of the core scheduler. For instance, there is a rule that if you need to lock multiple runqueue, on top of the overhead it might causes, you must always lock them in the correct order (i.e. the runqueue whose CPU id is the lowest must always be locked first

\par \textbf{Multi-policy support} SaaKM supports to have multiple policies loaded at the same time. When it registers, it is given an unique incremental id that is used by application to select their policy. We then maintain them in a linked list ordered by insertion order. When a new thread must be elected, we go through the list of policies starting from the head of the list. The first policy to return a non NULL thread stops the search and the thread is elected. For load balancing, we also go through the full list to make sure that all policies can perform their load balance.
\begin{table}[htbp]
        % \centering
        \caption{SaaKM thread states}
        \begin{tabular}{|l|l|}
        % \toprule
        \hline
        \textbf{State} & \textbf{Meaning} \\
        % \midrule
                \hline
                \texttt{SAAKM\_NOT\_QUEUED} & Not in a runqueue \\
                \hline
                \texttt{SAAKM\_MIGRATING} & Being migrated \\
                \hline
                \texttt{SAAKM\_RUNNING} & Running on a CPU \\
                \hline
                \texttt{SAAKM\_READY} & Ready to run and in a runqueue \\
                \hline
                \texttt{SAAKM\_READY\_TICK} & Became ready from a \textit{tick} event \\
                \hline
                \texttt{SAAKM\_BLOCKED} & Blocked and cannot run \\
                \hline
                \texttt{SAAKM\_TERMINATED} & Dead \\
                \hline
        \end{tabular}
        
\label{tab:saakm-states}
\end{table}
\pagebreak
% \par We implement a minimal (less than 1500 LoC) scheduling class that implements all of the scheduling functions requiered by the core scheduler. We place ourselves after the ext policy and before the idle one to not disturb higher priority scheduling classes (e.g RR, fair). We expose an API for the Linux kernel Modules (LKM) to enable them to register as scheduling policies. We store the policies in a linked list sorted by insertion date. Each policy is assigned a id that is unique and used by applications to select the policy they want to use. Figure ? shows the general architecture of SaaKM. \newline

\subsection{SaaKM workflow}
\par To register a policy, a module must implement a set of functions defined in the structure \texttt{saakm\_module\_routines}. It is composed of handlers that map to scheduling events. Table \ref{tab:saakm-callbacks} shows an excerpt of those events. Events are devided into two categories. Thread events consist of all events related to a thread (e.g thread is waking up, a new thread is created, ...) and core events that are related to core management (e.g core becoming idle, scheduling tick, ...). This distinction allow user to exactly know the path it is. For instance, take the \texttt{enqueue\_task} function from the \texttt{sched\_class} structure. X \todo{est-ce que c'est une bonne idée comme example ?} paths lead to its call. For instance, when a thread is waking up and need to be enqueued back on a runqueue or if a thread was just created. We hide this complexity behind the event and call the handlers at the right places. This way, the user does not need to differentiate paths, it just have to worry about the event currently happening. \note{ancienne version, bouger des phrases ailleurs}\newline
% \par \textit{Synchronizations} In order to hide the complexity of the synchronization mechanisms, we split the selection of a CPU and the actual enqueuing in two steps. The first one (\texttt{new\_prepare} and \texttt{unblock\_prepare}) must returns the id of the CPU on which the thread should run. Then comes the second step (\textit{new\_place} and \texttt{unblock\_place}) where the actual enqueuing takes place. It frees the user from knowing which locks are currently held and which ones it is allowed to take. It is needed for instance when a thread is created. \note{completer et réécrire} \newline
% \par \textit{Thread states} We overload thread states with ours to make it easier for the user to manage\note{?}. As we have the full control over our states, this allows us to check for wrongful transitions and provide debug feedback to the users. This ease the process of development and debugging. \newline
% \par \textit{Runqueues mamagement} Each policy must allocate its runqueues ...

% \par \textit{Thread migration} To handle thread migration, we introduce a new flag, OUSTED. This allows to discriminate between a dequeue coming from a blocking event from a migration. We then simulate a block/unblock sequence to let the policies remove the thread from its runqueue and insert it into the new one.

% \note{parler des etats des threads (et verifications via la machine à état), de comment on passe des données via les structures *\_event, peut etre un exemple du workflow (insertion usage etc), parler de la gestion des runqueues, la migration avec le flag OUSTED, gestion du load balancing,}

% \par We rely on the \textit{sched\_class} API presented above to implement a minimal core scheduler (less than 1500 LOC) that will only be used to register policies and call callbacks at the right places. To not interfer with the existing schedulers such as EEVDF or sched\_ext, we place our scheduling class right before the idle one.\\ Each policy must implement a set of functions that map to specific scheduling events (e.g wakeup, scheduling tick, ...) and then register itself to our scheduling class. To not interfer with the existing schedulers such as EEVDF or sched\_ext, we place our scheduling class right before the idle one. Figure ? shows the general architecture of SaaKM\\
% \par{} There are two types of events. The \textit{thread events} and \textit{core events}. Table \ref{tab:saakm-callbacks} shows some examples of these events and their descriptions. We devided the selection of a CPU and the actual enqueuing in the runqueue in two steps. This is needed when a task is newly created task is woken up for the first time. In a first time, it locks the task and calls \texttt{new\_prepare} which returns the CPU. At this point, our scheduler sees the task for the first time, it needs to initialize and allocated metadata for it. Only then, the runqueue of the CPU is locked and \texttt{new\_place} is called to actually enqueue the task. Thanks to this architecture, we are able to hide the complexity of the syncrhonization mechanisms to the user.

% \begin{table}[htbp]
% \caption{Table Type Styles}
% \begin{center}
% \begin{tabular}{|c|c|c|c|}
% \hline
% \textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
% \cline{2-4} 
% \textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
% \hline
% copy& More table copy$^{\mathrm{a}}$& &  \\
% \hline
% \multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
% \end{tabular}
% \label{tab1}
% \end{center}
% \end{table}
\begin{table}[htbp]
        % \centering
        \caption{Extract of \texttt{saakm\_module\_routines} functions}
        \begin{tabular}{|c|c|}
        % \toprule
        \hline
        \textbf{Function} & \textbf{Description} \\
        % \midrule
        \hline
                \textbf{Thread events} & \\
                \hline
                \texttt{new\_prepare(p)} & Return CPU id where \texttt{p} should run\\
                \texttt{new\_place(task, core)} & Insert \texttt{p} into \texttt{core} runqueue\\
                % \texttt{unblock\_place(task)} & Retourne l'identifiant du CPU où doit être insérée \texttt{task}\\
                % \texttt{unblock\_prepare(task)} & Insère \texttt{task} dans la runqueue de \texttt{core}\\
        
                \hline
                \textbf{Core events} & \\
                \hline
                \texttt{schedule(core)} & Called when a task must be elected\\
                \texttt{newly\_idle(core)} & Called right after \texttt{schedule}\\
        % \bottomrule
                \hline
                \textbf{Policy management events} & \\
                \texttt{init()} & Called to initialize policy\\
                \hline
        \end{tabular}
        
\label{tab:saakm-callbacks}
\end{table}

% \begin{itemize}
        % \item[-]{Task events} are all events related to task, such as : a blocking task, a task waking up.
        % \item[-]{Core events} are all events related to the CPUs, such as : a scheduling tick, a CPU becoming idle.
% \end{itemize}
% \par 
% In order to hide the complexity of the locks mecanisms and make it invisible to the user, we have to make the selection of a CPU and the actual enqueuing in two step. This is needed when a task \textit{p} is for waking up on CPU_0 but the scheduler decides that it should now run on CPU_1. Firstly, we need to lock CPU_0 to completly remove \textit{p} before we can enqueue it on CPU_1. \note{reecrire}